# -*- coding: utf-8 -*-
"""Copy of MLProject_ModelApplication.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WOSO0o55sYfuCN-FWZHvA3OHahQLqp74
"""

import numpy as np # linear algebra
import matplotlib.pyplot as plt
import random 
import sklearn
from sklearn.model_selection import train_test_split
import pandas as pd
import csv
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error
from sklearn.naive_bayes import GaussianNB
from sklearn.inspection import permutation_importance
from sklearn.model_selection import KFold
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from matplotlib.pyplot import figure

from google.colab import drive
drive.mount('/content/drive')

path_LP = '/content/drive/MyDrive/Machine_Learning_for_Engineers/FullFeatureData.csv'
path_MS = '/content/drive/MyDrive/FullFeatureData.csv'

data = pd.read_csv(path_MS)
label = data['Condition']
featdata = data.drop(columns = ['Subject Number','Condition'])

#Normalize data
normfd=(featdata-featdata.mean())/featdata.std()
print(np.shape(normfd)[1])

#create test and training data 
random.seed(10)
x_train, x_test, y_train, y_test = train_test_split(normfd,label,test_size=0.5)

# collect accuracies for all features with kfold cross-val 

full_models = []
full_acc = []
full_std = []
# all models 
dict_models = {
    'LG1': sklearn.linear_model.LogisticRegression(penalty='l1', solver='liblinear'),
    'LG': sklearn.linear_model.LogisticRegression(),
    'SVM': SVC(kernel = 'rbf'),
    'tree': DecisionTreeClassifier(),
    'SGD': SGDClassifier(loss="hinge", penalty="l2", max_iter=10),
    'knn': KNeighborsClassifier(n_neighbors=2),
    'naiveB': GaussianNB()
}
full_acc = []
temp_acc = []
temp_std = []
kf = KFold(n_splits=5, shuffle = True)
kf.get_n_splits(normfd)
for model in dict_models.values():
  for train_index, test_index in kf.split(normfd):
    X_train, X_test = normfd.loc[train_index], normfd.loc[test_index]
    Y_train, Y_test = label.loc[train_index], label.loc[test_index]

    modelfit = model.fit(X_train, Y_train)
    temp_acc = np.append(temp_acc, modelfit.score(X_test, Y_test))
  temp_avg_acc = np.mean(temp_acc)
  temp_std = np.std(temp_acc)
  full_acc = np.append(full_acc, temp_avg_acc)
  full_std = np.append(full_std, temp_std)
  temp_acc = []

print(full_acc)
from matplotlib import cm
paired_colors = cm.rainbow(np.linspace(0, 1, num=7))
model_names = [*dict_models]
figure(figsize=(6, 6), dpi=80)
plt.barh(model_names, full_acc*100, height=0.8, xerr = full_std*100, capsize = 4, align='center', color = paired_colors)
plt.title('Accuracy of Various Models Using all Features and KFold = 5 Cross-Validation')
plt.xlabel('Accuracy (%)')
plt.ylabel('Models')
plt.show()

avg_of_all_models = np.mean(full_acc)
print(avg_of_all_models)

# logistic regression to get feature weights
logr = LogisticRegression(random_state = 0)
logr.fit(x_train, y_train)
weights = logr.coef_
ind_asc = np.argsort(weights)[0] #indices of the features in ascending order of weights (importance)
ind_desc = np.argsort(-1*weights)[0]

# logistic regression 
n = x_train.shape[1]
# ind = np.arange(0, n) 
# np.random.shuffle(ind) #get random array of indices 
forward_score = np.zeros(n)
backward_score = np.zeros(n)
for i in range(0,n):
  #forward
  logr.fit(x_train.iloc[:,ind_desc[0:i+1]], y_train)
  forward_score[i] = logr.score(x_test.iloc[:,ind_desc[0:i+1]], y_test)
  
  #backward
  if i ==0:
    logr.fit(x_train.iloc[:,ind_desc], y_train)
    backward_score[i] = logr.score(x_test.iloc[:,ind_desc], y_test)
  else:
    logr.fit(x_train.iloc[:,ind_desc[0:-i]], y_train) 
    backward_score[i] = logr.score(x_test.iloc[:,ind_desc[0:-i]], y_test)
print(forward_score)
print(backward_score)

from matplotlib.pyplot import figure
from matplotlib import cm
# paired_colors = cm.rainbow(np.linspace(0, 1, num=14))
# figure(figsize=(6, 6), dpi=80)
plt.plot(range(1,15),forward_score[0:14]) # , color=paired_colors)
plt.ylabel('Accuracy')
plt.xlabel('Number of Features')
plt.title('Forward Logistic Regression with Highest Weight Features')
plt.show()

# paired t-test between features

from scipy import stats
# scipy.stats.ttest_rel(a, b, axis=0)
subj_num= data.loc[:,'Subject Number'].copy()
duplicates = []
from collections import Counter   
c = dict(Counter(subj_num))
for key, value in c.items():
  if value == 2:
    duplicates = np.append(duplicates, key)

d = len(duplicates)
f = x_train.shape[1];

index1 = []
index2 = []

for j in range(0, d):
  temp_index = np.where(duplicates[j] == subj_num)[0]
  index1 = np.append(index1, temp_index[0])
  index2 = np.append(index2, temp_index[1])
print(index1)
print(index2)

statistic = np.zeros(f)
pvalue = np.zeros(f)
for i in range(0,f):
  statistic[i], pvalue[i] = stats.ttest_rel(normfd.iloc[index1,i], normfd.iloc[index2,i])
sig_fig_ttest_ind = np.where(pvalue < .05)[0]
print(sig_fig_ttest_ind)
sig_fig_ttest = normfd.columns[sig_fig_ttest_ind]
print(sig_fig_ttest)

# adding L1 lasso to find best parameters

logreg = sklearn.linear_model.LogisticRegression(penalty='l1', solver='liblinear')
clf = logreg.fit(x_train, y_train)
predict = clf.predict(x_test)
score = clf.score(x_test, y_test)
trainscore = clf.score(x_train, y_train)
print(score)
print(trainscore)
#print(logreg.coef_)
nonzero = []
zero = []
for i, item in enumerate(logreg.coef_[0]):
  if item != 0:
    nonzero.append(i)
  if item == 0:
    zero.append(i)

print(nonzero)


sigparameters = x_test.columns[nonzero]
nonsigparam = x_test.columns[zero]
print(sigparameters.values)
#look at just acceration,just thigh, just left, etc.





#Accuracy Withouts those parameters
x_trainzeros = x_train[nonsigparam]
x_testzeros = x_test[nonsigparam]
logreg = sklearn.linear_model.LogisticRegression(penalty='l1', solver='liblinear')
clf = logreg.fit(x_trainzeros, y_train)
predict = clf.predict(x_testzeros)
score = clf.score(x_testzeros, y_test)
print(score)

nonzero = []
zero = []
for i, item in enumerate(logreg.coef_[0]):
  if item != 0:
    nonzero.append(i)
  if item == 0:
    zero.append(i)

print(nonzero)
sigparametersz = x_test.columns[nonzero]
print(sigparametersz)

#exploring the impact of dimensionality reduction on logistic regression classification
from sklearn.decomposition import PCA
x_data_sig = normfd
y_data_sig = label
xd_train, xd_test, yd_train, yd_test = train_test_split(x_data_sig,y_data_sig,test_size=0.3)
scorelist = []
exp_var = []
#determine how many of the significant parameters 
for k in range(1,11):
  pca = PCA(n_components=k)
  pcafit = pca.fit(xd_train)
  pca_train = pca.fit_transform(xd_train)
  PCA_Ev=np.sum(pcafit.explained_variance_)
  exp_var.append(PCA_Ev)
  n = pca.components_.shape[0]
  most_important = [np.abs(pca.components_[i]).argmax() for i in range(n)]
  feature_names = x_data_sig.columns
  most_important_names = [feature_names[most_important[i]] for i in range(n)]
  logreg = sklearn.linear_model.LogisticRegression()
  clf = logreg.fit(pca_train, yd_train)
  pca_test = pcafit.transform(xd_test)
  predict = clf.predict(pca_test)
  score = clf.score(pca_test, yd_test)
  scorelist.append(score)
print(scorelist)
plt.plot(range(1,11),scorelist)
print(most_important_names)

x_data = normfd[most_important_names]

kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(x_data)
acc = []
for train_index, test_index in kf.split(x_data):
    X_train, X_test = x_data.loc[train_index], x_data.loc[test_index]
    y_train, y_test = label.loc[train_index], label.loc[test_index]

    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear')
    regfit = logreg.fit(X_train,y_train)
    prediction=regfit.predict(X_test)
    accuracy = regfit.score(X_test,y_test)
    acc.append(accuracy)
    print(accuracy)
    msek5 = mean_squared_error(y_test, prediction)
    print('Mean squared error: '+str(msek5))
avg_acc_PCA_model = np.mean(acc)
std_acc_PCA_model = np.std(acc)
print(avg_acc_PCA_model)

plt.plot(range(1,11),exp_var)
plt.xlabel('Number of Principal Components')
plt.ylabel('Variance Explained')
plt.show()

from sklearn import tree
clf = tree.DecisionTreeClassifier()
clf = clf.fit(x_train, y_train)
result=clf.predict(x_test)
correct = 0
wrong = 0

for i in range(len(result)):
  if result[i] == y_test.iloc[i]:
    correct += 1
  else: 
    wrong += 1

accuracy = correct/len(result)*100
print('Accuracy:' +str(accuracy))

# show decision tree path

n_nodes = clf.tree_.node_count
children_left = clf.tree_.children_left
children_right = clf.tree_.children_right
feature = clf.tree_.feature
print(feature)
threshold = clf.tree_.threshold
impurity = clf.tree_.impurity
value = clf.tree_.value

def retrieve_branches(number_nodes, children_left_list, children_right_list):
    """Retrieve decision tree branches"""
    
    # Calculate if a node is a leaf
    is_leaves_list = [(False if cl != cr else True) for cl, cr in zip(children_left_list, children_right_list)]
    
    # Store the branches paths
    paths = []
    
    for i in range(number_nodes):
        if is_leaves_list[i]:
            # Search leaf node in previous paths
            end_node = [path[-1] for path in paths]

            # If it is a leave node yield the path
            if i in end_node:
                output = paths.pop(np.argwhere(i == np.array(end_node))[0][0])
                yield output

        else:
            
            # Origin and end nodes
            origin, end_l, end_r = i, children_left_list[i], children_right_list[i]

            # Iterate over previous paths to add nodes
            for index, path in enumerate(paths):
                if origin == path[-1]:
                    paths[index] = path + [end_l]
                    paths.append(path + [end_r])

            # Initialize path in first iteration
            if i == 0:
                paths.append([i, children_left[i]])
                paths.append([i, children_right[i]])

all_branches = list(retrieve_branches(n_nodes, children_left, children_right))
for index, branch in enumerate(all_branches):
    leaf_index = branch[-1]
    print(f'Branch: {index}, Path: {branch}')
    print(f'Gin {impurity[leaf_index]} at leaf node {branch[-1]}')
    print(f'Value: {value[leaf_index]}')
    print(f"Decision Rules: {[f'if X[:, {feature[elem]}] <= {threshold[elem]}' for elem in branch]}")
    print(f"---------------------------------------------------------------------------------------\n")

decision_tree_feat = []
feature = clf.tree_.feature
c = dict(Counter(feature))
for key, value in c.items():
    decision_tree_feat = np.append(decision_tree_feat, key)

print(decision_tree_feat)
# ['Max Gyro X (rad/s) left shank' 'Mean Accel Y (m/s^2) left shank'
 #'Max Accel Z (m/s^2) left shank' 'Minimum Accel X (m/s^2) right shank'
 #'Mean Accel Z (m/s^2) left shank' 'Minimum Accel Y (m/s^2) right shank']

#impact of increasing depth on accuracy


from sklearn import tree
from sklearn.metrics import accuracy_score

acc = []
for i in range(1,21):

  clf = tree.DecisionTreeClassifier(max_depth=i)
  clf = clf.fit(x_train, y_train)
  result=clf.predict(x_test)
  accuracy = accuracy_score(y_test,result)
  acc.append(accuracy)
print(acc)

groups = []
group_averages = []
standard_deviation = []
num_feats_traits = []

#Just Acceleration
accel_cols = [col for col in normfd.columns if 'Accel' in col]
x_data = normfd[accel_cols]
num_feats_traits = np.append(num_feats_traits, len(accel_cols))

kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(x_data)
acc = []
for train_index, test_index in kf.split(x_data):
    X_train, X_test = x_data.loc[train_index], x_data.loc[test_index]
    y_train, y_test = label.loc[train_index], label.loc[test_index]

    logreg = sklearn.linear_model.LogisticRegression(penalty='l1', solver='liblinear')
    regfit = logreg.fit(X_train,y_train)
    prediction=regfit.predict(X_test)
    accuracy = regfit.score(X_test,y_test)
    acc.append(accuracy)
    print(accuracy)
    msek5 = mean_squared_error(y_test, prediction)
    print('Mean squared error: '+str(msek5))
avg_acc_accel = np.mean(acc)
groups.append('Acceleration')
group_averages.append(avg_acc_accel)
standard_deviation.append(np.std(acc))

#Just Gyro
gyro_cols = [col for col in normfd.columns if 'Gyro' in col]
x_data = normfd[gyro_cols]
num_feats_traits = np.append(num_feats_traits, len(gyro_cols))

kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(x_data)
acc = []
for train_index, test_index in kf.split(x_data):
    X_train, X_test = x_data.loc[train_index], x_data.loc[test_index]
    y_train, y_test = label.loc[train_index], label.loc[test_index]

    logreg = sklearn.linear_model.LogisticRegression(penalty='l1', solver='liblinear')
    regfit = logreg.fit(X_train,y_train)
    prediction=regfit.predict(X_test)
    accuracy = regfit.score(X_test,y_test)
    acc.append(accuracy)
    print(accuracy)
    msek5 = mean_squared_error(y_test, prediction)
    print('Mean squared error: '+str(msek5))
avg_acc_gyro = np.mean(acc)
groups.append('Gyroscope')
group_averages.append(avg_acc_gyro)
standard_deviation.append(np.std(acc))

#Just X
X_cols = [col for col in normfd.columns if 'X' in col]
x_data = normfd[X_cols]
num_feats_traits = np.append(num_feats_traits, len(X_cols))
print(num_feats_traits)

print(x_data.columns)
kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(x_data)
acc = []
for train_index, test_index in kf.split(x_data):
    X_train, X_test = x_data.loc[train_index], x_data.loc[test_index]
    y_train, y_test = label.loc[train_index], label.loc[test_index]

    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear')
    regfit = logreg.fit(X_train,y_train)
    prediction=regfit.predict(X_test)
    accuracy = regfit.score(X_test,y_test)
    acc.append(accuracy)
    print(accuracy)
    msek5 = mean_squared_error(y_test, prediction)
    print('Mean squared error: '+str(msek5))
avg_acc_x = np.mean(acc)
groups.append('X Values')
group_averages.append(avg_acc_x)
standard_deviation.append(np.std(acc))

#Just Y
Y_cols = [col for col in normfd.columns if 'Y' in col]
x_data = normfd[Y_cols]
num_feats_traits = np.append(num_feats_traits, len(Y_cols))
print(num_feats_traits)

kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(x_data)
acc = []
for train_index, test_index in kf.split(x_data):
    X_train, X_test = x_data.loc[train_index], x_data.loc[test_index]
    y_train, y_test = label.loc[train_index], label.loc[test_index]

    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear')
    regfit = logreg.fit(X_train,y_train)
    prediction=regfit.predict(X_test)
    accuracy = regfit.score(X_test,y_test)
    acc.append(accuracy)
    print(accuracy)
    msek5 = mean_squared_error(y_test, prediction)
    print('Mean squared error: '+str(msek5))
avg_acc_y = np.mean(acc)
groups.append('Y Values')
group_averages.append(avg_acc_y)
standard_deviation.append(np.std(acc))

#Just Z
Z_cols = [col for col in normfd.columns if 'Z' in col]
x_data = normfd[Z_cols]
num_feats_traits = np.append(num_feats_traits, len(Z_cols))
print(num_feats_traits)

kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(x_data)
acc = []
for train_index, test_index in kf.split(x_data):
    X_train, X_test = x_data.loc[train_index], x_data.loc[test_index]
    y_train, y_test = label.loc[train_index], label.loc[test_index]

    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear')
    regfit = logreg.fit(X_train,y_train)
    prediction=regfit.predict(X_test)
    accuracy = regfit.score(X_test,y_test)
    acc.append(accuracy)
    print(accuracy)
    msek5 = mean_squared_error(y_test, prediction)
    print('Mean squared error: '+str(msek5))
avg_acc_z = np.mean(acc)
groups.append('Z Values')
group_averages.append(avg_acc_z)
standard_deviation.append(np.std(acc))

#Just Shank
shank_cols = [col for col in normfd.columns if 'shank' in col]
x_data = normfd[shank_cols]
num_feats_traits = np.append(num_feats_traits, len(shank_cols))
print(num_feats_traits)

kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(x_data)
acc = []
for train_index, test_index in kf.split(x_data):
    X_train, X_test = x_data.loc[train_index], x_data.loc[test_index]
    y_train, y_test = label.loc[train_index], label.loc[test_index]

    logreg = sklearn.linear_model.LogisticRegression( solver='liblinear')
    regfit = logreg.fit(X_train,y_train)
    prediction=regfit.predict(X_test)
    accuracy = regfit.score(X_test,y_test)
    acc.append(accuracy)
    print(accuracy)
    msek5 = mean_squared_error(y_test, prediction)
    print('Mean squared error: '+str(msek5))
avg_acc_shank = np.mean(acc)
groups.append('Shank Data')
group_averages.append(avg_acc_shank)
standard_deviation.append(np.std(acc))

#Just Thigh
thigh_cols = [col for col in normfd.columns if 'thigh' in col]
x_data = normfd[thigh_cols]
num_feats_traits = np.append(num_feats_traits, len(thigh_cols))
print(num_feats_traits)

kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(x_data)
acc = []
for train_index, test_index in kf.split(x_data):
    X_train, X_test = x_data.loc[train_index], x_data.loc[test_index]
    y_train, y_test = label.loc[train_index], label.loc[test_index]

    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear')
    regfit = logreg.fit(X_train,y_train)
    prediction=regfit.predict(X_test)
    accuracy = regfit.score(X_test,y_test)
    acc.append(accuracy)
    print(accuracy)
    msek5 = mean_squared_error(y_test, prediction)
    print('Mean squared error: '+str(msek5))
avg_acc_thigh = np.mean(acc)
groups.append('Thigh Data')
group_averages.append(avg_acc_thigh)
standard_deviation.append(np.std(acc))

#Just mean/avg
avg_cols = [col for col in normfd.columns if 'avg' in col]
mean_cols = [col for col in normfd.columns if 'Mean' in col]
avg_mean_cols = np.concatenate((avg_cols, mean_cols))
print(avg_mean_cols)
x_data = normfd[avg_mean_cols]
num_feats_traits = np.append(num_feats_traits, len(avg_mean_cols))
print(num_feats_traits)

kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(x_data)
acc = []
for train_index, test_index in kf.split(x_data):
    X_train, X_test = x_data.loc[train_index], x_data.loc[test_index]
    y_train, y_test = label.loc[train_index], label.loc[test_index]

    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear')
    regfit = logreg.fit(X_train,y_train)
    prediction=regfit.predict(X_test)
    accuracy = regfit.score(X_test,y_test)
    acc.append(accuracy)
    print(accuracy)
    msek5 = mean_squared_error(y_test, prediction)
    print('Mean squared error: '+str(msek5))
avg_acc_mean = np.mean(acc)
groups.append('Average Values')
group_averages.append(avg_acc_mean)
standard_deviation.append(np.std(acc))

#calling just the temporal parameters calculated columns columns
temporal_cols = [col for col in normfd.columns if '[' in col]
x_data = normfd[temporal_cols]
num_feats_traits = np.append(num_feats_traits, len(temporal_cols))
print(num_feats_traits)

kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(x_data)
acc = []
for train_index, test_index in kf.split(x_data):
    X_train, X_test = x_data.loc[train_index], x_data.loc[test_index]
    y_train, y_test = label.loc[train_index], label.loc[test_index]

    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear')
    regfit = logreg.fit(X_train,y_train)
    prediction=regfit.predict(X_test)
    accuracy = regfit.score(X_test,y_test)
    acc.append(accuracy)
    print(accuracy)
    msek5 = mean_squared_error(y_test, prediction)
    print('Mean squared error: '+str(msek5))
avg_acc_alltempparams = np.mean(acc)
groups.append('Spatio-temporal Data')
group_averages.append(avg_acc_alltempparams)
standard_deviation.append(np.std(acc))

#calling just percentage of gait cycle columns (normalized by GCT)
temporal_cols = [col for col in normfd.columns if '%' in col]
x_data = normfd[temporal_cols]
num_feats_traits = np.append(num_feats_traits, len(temporal_cols))
print(num_feats_traits)

kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(x_data)
acc = []
for train_index, test_index in kf.split(x_data):
    X_train, X_test = x_data.loc[train_index], x_data.loc[test_index]
    y_train, y_test = label.loc[train_index], label.loc[test_index]

    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear')
    regfit = logreg.fit(X_train,y_train)
    prediction=regfit.predict(X_test)
    accuracy = regfit.score(X_test,y_test)
    acc.append(accuracy)
    print(accuracy)
    msek5 = mean_squared_error(y_test, prediction)
    print('Mean squared error: '+str(msek5))
avg_acc_normGCTtempparams = np.mean(acc)
groups.append('Data Normalized by GCT')
group_averages.append(avg_acc_normGCTtempparams)
standard_deviation.append(np.std(acc))

#calling just means of temporal parameters columns
avgtemporal_cols = [col for col in normfd.columns if 'avg.' in col]
x_data = normfd[avgtemporal_cols]
num_feats_traits = np.append(num_feats_traits, len(avgtemporal_cols))
print(num_feats_traits)

kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(x_data)
acc = []
for train_index, test_index in kf.split(x_data):
    X_train, X_test = x_data.loc[train_index], x_data.loc[test_index]
    y_train, y_test = label.loc[train_index], label.loc[test_index]

    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear')
    regfit = logreg.fit(X_train,y_train)
    prediction=regfit.predict(X_test)
    accuracy = regfit.score(X_test,y_test)
    acc.append(accuracy)
    print(accuracy)
    msek5 = mean_squared_error(y_test, prediction)
    print('Mean squared error: '+str(msek5))
avg_acc_meantempparams = np.mean(acc)
groups.append('Average Temporal Data')
group_averages.append(avg_acc_meantempparams)
standard_deviation.append(np.std(acc))

#calling just limp temporal parameters columns
limptemporal_cols = [col for col in normfd.columns if 'Limp' in col]
x_data = normfd[limptemporal_cols]
num_feats_traits = np.append(num_feats_traits, len(limptemporal_cols))
print(num_feats_traits)

kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(x_data)
acc = []
for train_index, test_index in kf.split(x_data):
    X_train, X_test = x_data.loc[train_index], x_data.loc[test_index]
    y_train, y_test = label.loc[train_index], label.loc[test_index]

    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear')
    regfit = logreg.fit(X_train,y_train)
    prediction=regfit.predict(X_test)
    accuracy = regfit.score(X_test,y_test)
    acc.append(accuracy)
    print(accuracy)
    msek5 = mean_squared_error(y_test, prediction)
    print('Mean squared error: '+str(msek5))
avg_acc_limp = np.mean(acc)
groups.append('Limp Data')
group_averages.append(avg_acc_limp)
standard_deviation.append(np.std(acc))

#Calling just GCT parameters
GCTtemporal_cols = [col for col in normfd.columns if '[s]' in col]
x_data = normfd[GCTtemporal_cols]
num_feats_traits = np.append(num_feats_traits, len(GCTtemporal_cols))
print(num_feats_traits)

kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(x_data)
acc = []
for train_index, test_index in kf.split(x_data):
    X_train, X_test = x_data.loc[train_index], x_data.loc[test_index]
    y_train, y_test = label.loc[train_index], label.loc[test_index]
    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear')
    regfit = logreg.fit(X_train,y_train)
    prediction=regfit.predict(X_test)
    accuracy = regfit.score(X_test,y_test)
    acc.append(accuracy)
    print(accuracy)
    msek5 = mean_squared_error(y_test, prediction)
    print('Mean squared error: '+str(msek5))
avg_acc_GCT = np.mean(acc)
groups.append('Gait Cycle Time Data')
group_averages.append(avg_acc_GCT)
standard_deviation.append(np.std(acc))

#calling just swing time 
swingtemporal_cols = [col for col in normfd.columns if 'Swing' in col]
x_data = normfd[swingtemporal_cols]
num_feats_traits = np.append(num_feats_traits, len(swingtemporal_cols))
print(num_feats_traits)

kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(x_data)
acc = []
for train_index, test_index in kf.split(x_data):
    X_train, X_test = x_data.loc[train_index], x_data.loc[test_index]
    y_train, y_test = label.loc[train_index], label.loc[test_index]
    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear')
    regfit = logreg.fit(X_train,y_train)
    prediction=regfit.predict(X_test)
    accuracy = regfit.score(X_test,y_test)
    acc.append(accuracy)
    print(accuracy)
    msek5 = mean_squared_error(y_test, prediction)
    print('Mean squared error: '+str(msek5))
avg_acc_swing = np.mean(acc)
groups.append('Swing Data')
group_averages.append(avg_acc_swing)
standard_deviation.append(np.std(acc))

#calling just double support
doubletemporal_cols = [col for col in normfd.columns if 'Double' in col]
x_data = normfd[doubletemporal_cols]
num_feats_traits = np.append(num_feats_traits, len(doubletemporal_cols))
print(num_feats_traits)

kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(x_data)
acc = []
for train_index, test_index in kf.split(x_data):
    X_train, X_test = x_data.loc[train_index], x_data.loc[test_index]
    y_train, y_test = label.loc[train_index], label.loc[test_index]
    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear')
    regfit = logreg.fit(X_train,y_train)
    prediction=regfit.predict(X_test)
    accuracy = regfit.score(X_test,y_test)
    acc.append(accuracy)
    print(accuracy)
    msek5 = mean_squared_error(y_test, prediction)
    print('Mean squared error: '+str(msek5))
avg_acc_doublesupport = np.mean(acc)
groups.append('Double Support Data')
group_averages.append(avg_acc_doublesupport)
standard_deviation.append(np.std(acc))

feat_number = []
feat_accuracy = []
feat_std = []
feature_extraction_method = []

X_data = normfd
kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(normfd)
acc = []
for train_index, test_index in kf.split(X_data):
    X_train, X_test = X_data.loc[train_index], X_data.loc[test_index]
    Y_train, Y_test = label.loc[train_index], label.loc[test_index]
    logreg = sklearn.linear_model.LogisticRegression(penalty='l1', solver='liblinear')
    clf = logreg.fit(X_train, Y_train)
    predict = clf.predict(X_test)
    score = clf.score(X_test, Y_test)
    acc.append(score)
    print(score)
feature_extraction_method.append('No Regularization or Feature Reduction ')
feat_accuracy.append(np.mean(acc))
feat_std.append(np.std(acc))
feat_number.append(np.shape(normfd)[1])

X_data = normfd[sig_fig_ttest]
kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(normfd)
acc = []
for train_index, test_index in kf.split(X_data):
    X_train, X_test = X_data.loc[train_index], X_data.loc[test_index]
    Y_train, Y_test = label.loc[train_index], label.loc[test_index]
    logreg = sklearn.linear_model.LogisticRegression(penalty='l1', solver='liblinear')
    clf = logreg.fit(X_train, Y_train)
    predict = clf.predict(X_test)
    score = clf.score(X_test, Y_test)
    acc.append(score)
    print(score)
feature_extraction_method.append('P<0.05 on a Normal T-Test')
feat_accuracy.append(np.mean(acc))
feat_std.append(np.std(acc))
feat_number.append(len(sig_fig_ttest))

#your determined significant features
x_data = normfd[names]
kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(x_data)
acc = []
for train_index, test_index in kf.split(x_data):
    X_train, X_test = x_data.loc[train_index], x_data.loc[test_index]
    y_train, y_test = label.loc[train_index], label.loc[test_index]

    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear')
    regfit = logreg.fit(X_train,y_train)
    prediction=regfit.predict(X_test)
    accuracy = regfit.score(X_test,y_test)
    acc.append(accuracy)
    print(accuracy)
    msek5 = mean_squared_error(y_test, prediction)
    print('Mean squared error: '+str(msek5))
avg_acc_sigparam = np.mean(acc)
feature_extraction_method.append('Top 25 Features without Regularization')
feat_accuracy.append(np.mean(acc))
feat_std.append(np.std(acc))
feat_number.append(len(names))

#my determined sig parameters with L1 lasso of all parameters
x_data = normfd[sigparameters]

kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(x_data)
acc = []
for train_index, test_index in kf.split(x_data):
    X_train, X_test = x_data.loc[train_index], x_data.loc[test_index]
    y_train, y_test = label.loc[train_index], label.loc[test_index]

    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear')
    regfit = logreg.fit(X_train,y_train)
    prediction=regfit.predict(X_test)
    accuracy = regfit.score(X_test,y_test)
    acc.append(accuracy)
    print(accuracy)
    msek5 = mean_squared_error(y_test, prediction)
    print('Mean squared error: '+str(msek5))
avg_acc_sigparam = np.mean(acc)
feature_extraction_method.append('L1 Regularization for 1 train/test set')
feat_accuracy.append(np.mean(acc))
feat_std.append(np.std(acc))
feat_number.append(len(sigparameters))
print(avg_acc_sigparam)

from collections import Counter
kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(normfd)
acc = []
nonzero = []
for train_index, test_index in kf.split(normfd):
    X_train, X_test = normfd.loc[train_index], normfd.loc[test_index]
    Y_train, Y_test = label.loc[train_index], label.loc[test_index]
    logreg = sklearn.linear_model.LogisticRegression(penalty='l1', solver='liblinear')
    clf = logreg.fit(X_train, Y_train)
    predict = clf.predict(X_test)
    score = clf.score(X_test, Y_test)
    print(score)

    for i, item in enumerate(logreg.coef_[0]):
        if item != 0:
            nonzero.append(i)
ints = Counter(nonzero)
print(ints.most_common())
sig_overlap=[]
for i in range(0,len(ints.most_common())):
  if ints.most_common()[i][1] == 5:
    sig_overlap.append(ints.most_common()[i][0])
commonsignificance = normfd.columns[sig_overlap]
print(commonsignificance)

X_data = normfd[commonsignificance]
kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(normfd)
acc = []
for train_index, test_index in kf.split(X_data):
    X_train, X_test = X_data.loc[train_index], X_data.loc[test_index]
    Y_train, Y_test = label.loc[train_index], label.loc[test_index]
    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear')
    clf = logreg.fit(X_train, Y_train)
    predict = clf.predict(X_test)
    score = clf.score(X_test, Y_test)
    acc.append(score)
    print(score)
avg_acc_common_feats = np.mean(acc)
print(avg_acc_common_feats)
feature_extraction_method.append('L1 regularization with 5 Fold Cross Validation')
feat_accuracy.append(np.mean(acc))
feat_std.append(np.std(acc))
feat_number.append(len(commonsignificance))
print(avg_acc_common_feats)

standard_dev_common_feats = np.std(acc)

mean_values = [avg_acc_common_feats, avg_acc_PCA_model]
labels = ['K-Fold Feature Extraction','PCA']
error = [standard_dev_common_feats,std_acc_PCA_model]
plt.bar(labels, mean_values,yerr=error,color=['r','g'])
plt.ylabel('Accuracy')
plt.xlabel('Feature Reduction Type')
plt.show()

print(commonsignificance)
print(len(commonsignificance))
print(logreg.coef_)
commonsignificance
print(np.shape(final_sig_param))
weights = logreg.coef_.reshape(-1,1)
idx = pd.Index(commonsignificance, name='Features')
df = pd.DataFrame(weights,index=idx, columns = ['Weight'])

paired_colors = cm.rainbow(np.linspace(0, 1, num=10))

print(df)
df = df.abs()
ordered_df = df.sort_values(by=['Weight'])
ordered_df.plot.bar(color=paired_colors)
plt.ylabel("Feature Weight")
plt.show()

html = df.to_html()
  
# write html to file
text_file = open("index.html", "w")
text_file.write(html)
text_file.close()

#regular logistic

x_data = normfd

kf = KFold(n_splits=5,shuffle=True)
kf.get_n_splits(x_data)
acc = []
for train_index, test_index in kf.split(x_data):
    X_train, X_test = x_data.loc[train_index], x_data.loc[test_index]
    y_train, y_test = label.loc[train_index], label.loc[test_index]

    logreg = sklearn.linear_model.LogisticRegression(penalty = 'l2', solver='liblinear')
    regfit = logreg.fit(X_train,y_train)
    prediction=regfit.predict(X_test)
    accuracy = regfit.score(X_test,y_test)
    acc.append(accuracy)
    print(accuracy)
    msek5 = mean_squared_error(y_test, prediction)
    print('Mean squared error: '+str(msek5))
avg_acc_sigparam = np.mean(acc)
feature_extraction_method.append('L1 Regularization for 1 train/test set')
feat_accuracy.append(np.mean(acc))
feat_std.append(np.std(acc))
feat_number.append(len(sigparameters))
print(avg_acc_sigparam)

from matplotlib.pyplot import figure
from matplotlib import cm
paired_colors = cm.rainbow(np.linspace(0, 1, num=15))
figure(figsize=(6, 6), dpi=80)
print(feat_accuracy)
plt.barh(feature_extraction_method,feat_accuracy,height=0.8,xerr=feat_std,capsize=4, align='center',color=paired_colors)
plt.legend()
plt.ylabel('Reduction Method')
plt.xlabel('Accuracy')
plt.title('Model Accuracy for Each Reduction Method')
plt.show()

post_mean = []
pre_mean = []
for i in range(len(normfd['Swing time avg. [% GCT] left'])):
  if label[i] == 1:
    post_mean.append(normfd['Swing time avg. [% GCT] left'][i])
  if label[i] == 0:
    pre_mean.append(normfd['Swing time avg. [% GCT] left'][i])
print(np.mean((post_mean))
print(np.mean(pre_mean))

import numpy as np
from matplotlib import cm
paired_colors = cm.rainbow(np.linspace(0, 1, num=15))

print(num_feats_traits)
groups = [groups[0], groups[1], groups[5], groups[6], groups[2], groups[3], groups[4], groups[7], groups[8], groups[9], groups[10], groups[11], groups[12], groups[13], groups[14]]
group_averages = [group_averages[0], group_averages[1], group_averages[5], group_averages[6], group_averages[2], group_averages[3], group_averages[4], group_averages[7], group_averages[8], group_averages[9], group_averages[10], group_averages[11], group_averages[12], group_averages[13], group_averages[14]]
print(group_averages)
print(groups)

from matplotlib.pyplot import figure
print(np.shape(group_averages))
figure(figsize=(6, 6), dpi=80)
for i in range(len(group_averages)):
  group_averages[i] = group_averages[i]*100
  standard_deviation[i] = standard_deviation[i]*100
print(groups)
print(group_averages)
plt.barh(groups, group_averages, height=0.8,xerr=standard_deviation,capsize=4, align='center',color=paired_colors)
plt.xlabel('Accuracy (%)')
plt.ylabel('Features')
plt.show()
plt.show()